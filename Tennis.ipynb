{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "In this notebook, I'll train an agent to solve the Tennis Environment using Unity ML-Agents.\n",
    "\n",
    "---\n",
    "\n",
    "# Index\n",
    "\n",
    "- [1. Setup the Environment](#1.-Setup-the-Environment)\n",
    "- [2. Start the Environment](#2.-Start-the-Environment)\n",
    "- [3. Examine the State and Action Spaces](#3.-Examine-the-State-and-Action-Spaces)\n",
    "- [4. Define helper functions to training session](#4.-Define-helper-functions-to-training-session)\n",
    "- [5. Training the Agent](#5.-Training-the-Agent)\n",
    "- [6. Test Agent on Environment](#6.-Test-Agent-on-Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the Environment\n",
    "\n",
    "<img align=\"left\" width=\"150\" src=\"https://www.nclouds.com/img/services/toolkit/sagemaker.png\"/>\n",
    "\n",
    "This notebook was developed on AWS SageMaker.\n",
    "\n",
    "The kernel used is **conda_python3**\n",
    "\n",
    "To setup this environment on SageMaker you need to run the next 3 cells.\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "from agent import Agent\n",
    "\n",
    "env_path = \"envs/Tennis_Linux/Tennis.x86_64\" # Linux\n",
    "#env_path = \"envs/Tennis_Windows_x86_64/Tennis.exe\" # Windows\n",
    "\n",
    "env = UnityEnvironment(file_name=env_path, no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define helper functions to training session\n",
    "\n",
    "Here we'll create a function that can be very helpful to teach the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_trainer(agents, n_episodes=2000, max_time_step=None, score_deque=100, print_range=10, early_stop=30, add_noise=False, save_model=True, verbose=False):\n",
    "    \"\"\"Deep Q-Learning trainer.\n",
    "    Params\n",
    "    ======\n",
    "        agents (list): List of Agent Objects\n",
    "        n_episodes (int): maximum number of training episodes.\n",
    "        max_time_step (int or None): The max time step per episode, if None, the agent will training until environment is done.\n",
    "        scores_deque (int): The len of score deque.\n",
    "        print_range (int): range to print partials results.\n",
    "        early_stop (int): Stop training when achieve a defined score respecting 10 min n_episodes.\n",
    "        add_noise (bool): If True, we'll reset at each step and add_noise at each action call.\n",
    "        save_model (bool): If True, we'll save the model weights when we solve the environment\n",
    "        verbose (bool): If verbose true, we'll print some infos on console.\n",
    "    \"\"\"\n",
    "    scores_window = deque(maxlen=score_deque)\n",
    "    scores = np.zeros(num_agents)\n",
    "    scores_episode = []\n",
    "    scores_range = []\n",
    "    \n",
    "    for ep in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores = np.zeros(num_agents)\n",
    "        time_step = 0\n",
    "        training = True\n",
    "        \n",
    "        if add_noise:\n",
    "            for agent in agents:\n",
    "                agent.reset()\n",
    "\n",
    "        while training:\n",
    "            actions = np.array([agents[i].act(states[i], add_noise) for i in range(num_agents)])\n",
    "            env_info = env.step(actions)[brain_name]        # send the action to the environment\n",
    "            next_states = env_info.vector_observations     # get the next state\n",
    "            rewards = env_info.rewards                     # get the reward\n",
    "            dones = env_info.local_done        \n",
    "            \n",
    "            for i in range(num_agents):\n",
    "                agents[i].step(states[i], actions[i], rewards[i], next_states[i], dones[i]) \n",
    " \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            time_step += 1\n",
    "            \n",
    "            if isinstance(max_time_step, (int, float)):\n",
    "                if time_step >= max_time_step:\n",
    "                    training = False\n",
    "                    \n",
    "            if np.any(dones):\n",
    "                break \n",
    "            \n",
    "        score = np.max(scores)\n",
    "        scores_window.append(score)            \n",
    "        scores_episode.append(score)\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\rEpisode {} || Avg Score: {:.2f} || Max Score: {:.2f} || Avg Score (last {} episodes): {:.2f}'.format(ep, score, np.max(scores), score_deque, round(np.mean(scores_window),2)), end=\"\")\n",
    "            if ep % print_range == 0:\n",
    "                scores_range.append(np.mean(scores_window))\n",
    "                print('\\rEpisode {} || Avg Score: {:.2f} || Max Score: {:.2f} || Avg Score (last {} episodes): {:.2f}'.format(ep, score, np.max(scores), score_deque, round(np.mean(scores_window),2)))\n",
    "        if np.mean(scores_window) >= early_stop:\n",
    "            if verbose:\n",
    "                print('\\nEnvironment solved in {:d} episodes! || Avg Score: {:.2f}'.format(ep, round(np.mean(scores_window),2)))\n",
    "            if save_model:\n",
    "                Agent.save_model(params=agents[0].get_params())\n",
    "                \n",
    "            break\n",
    "            \n",
    "    return {\"scores_episode\": scores_episode, \"scores_range\": scores_range}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Agent\n",
    "\n",
    "In the next code cells, we will train the Agent to work on environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "agents = [] \n",
    "for i in range(num_agents):\n",
    "    agents.append(Agent(state_size=state_size, action_size=action_size, random_seed=399, memory_size=int(1e6),\n",
    "                        batch_size=32, gamma=0.99, tau=1e-3, lr_actor=1e-4, lr_critic=1e-4, weight_decay=0.0,\n",
    "                        actor_units=(256, 128), critic_units=(256, 128), action_range=(-1, 1)))\n",
    "    \n",
    "training_results = rl_trainer(agents, n_episodes=4000, print_range=100, early_stop=0.5,\n",
    "                              add_noise=True, save_model=True, verbose=True)\n",
    "\n",
    "agent = agents[0]\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(training_results[\"scores\"])), training_results[\"scores\"])\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent.load_model()\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Agent on Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "score = 0\n",
    "\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    score += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
