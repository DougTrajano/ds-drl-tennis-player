{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm\n",
    "\n",
    "The algorithm that I choosed to use on this environment is DDPG (Deep Deterministic Policy Gradient), you can see more about this algorithm [here](http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Spring18/ujjawal/DDPG-Algorithm.pdf)\n",
    "\n",
    "See below the architecture of this algorithm\n",
    "\n",
    "![](images/DDPG_architecture.png)\n",
    "\n",
    "DDPG is an **Advantage Actor Critic Method** that combines Policy-based and Value-based in the same agent. See below a simplest explanation about how it's works.\n",
    "\n",
    "![](images/actor-critic_explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameters were chosen based on training experiments seeking a score greater than 5 in the least episodes possible.\n",
    "\n",
    "At the end, our hyperparameters are:\n",
    "\n",
    "```\n",
    "PENDENTE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks architectures\n",
    "\n",
    "We have two Neural Networks for this agent, Actor and Critic. See below the architecture of each of these neural networks.\n",
    "\n",
    "### Actor Neural Network\n",
    "\n",
    "```\n",
    "PENDENTE\n",
    "```\n",
    "\n",
    "### Critic Neural Network\n",
    "\n",
    "```\n",
    "PENDENTE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of Rewards\n",
    "\n",
    "See below the score graph and how much episodes was needed to solve the environment.\n",
    "\n",
    "![](images/agents_score.png)\n",
    "\n",
    "The complete training history is available on [Tennis.ipynb](Tennis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for future work\n",
    "\n",
    "I have some ideas for future work to improve the agent's performance.\n",
    "\n",
    "- New combinations of hyperparameters.\n",
    "- Implement [Prioritized Experience Replay](https://arxiv.org/pdf/1511.05952.pdf)\n",
    "- Change the Exploration Policy to Gaussian Noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
